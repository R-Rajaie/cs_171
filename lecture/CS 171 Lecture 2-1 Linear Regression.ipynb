{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa78574b-d9db-4449-b800-0aa9b1460993",
   "metadata": {},
   "source": [
    "# Linear Regression as a Machine Learning Model\n",
    "In this notebook, we will consider linear regression and how it might be approached as a machine learning model.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "At the end of this notebook, you should be able to\n",
    "1. envision linear regression as an iterative process.\n",
    "2. define the mean square error cost function and compute its gradient\n",
    "3. describe how gradient descent is used to final optimal model parameters.\n",
    "\n",
    "**Import modules**\n",
    "\n",
    "Begin by importing the modules to be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5b778-fdba-4ce4-b322-aa326262b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90628864-f7fb-42f5-bf90-a66254d0f7e9",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "In its simplest form, linear regression refers to fitting a simple line to data. In our early math classes, we learn that a line has the following form:\n",
    "\n",
    "$$\n",
    "y = mx+b\n",
    "$$\n",
    "\n",
    "where $x$ is some independent data, $y$ is some dependent data, $m$ is the slope and $b$ is the intercept. \n",
    "\n",
    "For a concrete example, we will consider the trend in CO$_2$ concentration in the atmosphere as measured at the Mauna Loa Observatory on the Mauna Loa volcano in Hilo Hawaii:\n",
    "\n",
    "\n",
    "<img src=\"Mauna_Loa_Observatory.jpg\" alt=\"Mauna Loa Observatory\" width=\"500\">\n",
    "\n",
    "Photo Credit: Johnathon Kingston, [National Geographic](https://education.nationalgeographic.org/resource/mauna-loa-observatory/).\n",
    "\n",
    "\n",
    "The record of CO$_2$ concentration collected at Mauna Loa, sometimes referred to as the \"Keeling Curve\" for its founding scientist, is a multi-decadal record that is available from the Scripps Institute of Oceanography [HERE](https://keelingcurve.ucsd.edu/). The video [HERE](https://youtu.be/dXBzFNEwoj8) for information on how the data is collected.\n",
    "\n",
    "Here, we will work the monthly data in this dataset, which is available in the `monthly_in_situ_co2_mlo.csv` file. Let's read that in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d2134-a537-4c17-9ce3-4ca1cf9e608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset with pandas\n",
    "df = pd.read_csv('monthly_in_situ_co2_mlo.csv', skiprows=64)\n",
    "\n",
    "# filter out null values stored as -99\n",
    "df = df[df.iloc[:, 4] >0]\n",
    "\n",
    "# the decimal year information is in the 4th column\n",
    "x = df.iloc[:, 3]\n",
    "\n",
    "# the CO2 information is in the 5th column\n",
    "y = df.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef5e95-f6d2-4364-b495-93b2dc9cec55",
   "metadata": {},
   "source": [
    "Let's take a look at what this data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcd8bb-ad5a-4214-b464-ce30046b5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x,y,'k.')\n",
    "plt.xlabel('x (year)')\n",
    "plt.ylabel('y (CO$_2$ concentration, ppm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf78d4-2b94-4d7e-8e1e-3edaee90d4ad",
   "metadata": {},
   "source": [
    "Interesting! There's a lot going on in this plot. For the purposes of this demo, let's subset our data to be during the period 2010-2020 (don't worry, we'll come back to the full dataset in a subsequent next notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fc177-4d05-400a-bf42-a9b77280e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to a given time range\n",
    "df_subset = df[(x>=2010) & (x<=2020)]\n",
    "\n",
    "# redefine x and y\n",
    "x = np.array(df_subset.iloc[:,3])\n",
    "y = np.array(df_subset.iloc[:,4])\n",
    "\n",
    "# remove the first value of y\n",
    "x = x-2010\n",
    "y = y-y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9ebdf-a2d2-452a-b3a5-6d774386a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x,y,'k.')\n",
    "plt.xlabel('x (years since 2010)')\n",
    "plt.ylabel('y (CO$_2$ concentration relative to 2010, ppm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2a10e-d0fb-4c01-bec8-dc698fa10853",
   "metadata": {},
   "source": [
    "Visually looking at the data, we can see that the curve has a faily consistent upward trend amid a seasonal trend. One question we may be interested in is: how much has the CO$_2$ concentration been increasing each year?\n",
    "\n",
    "## Linear Regression with `numpy`\n",
    "\n",
    "We can fit a polynomial to this data using `numpy` as follows to find the slope of the line of best fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41159f-45e6-4431-ad23-9920ce5da3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09e6c145-d663-4a2f-aba4-c8b38e02f1f6",
   "metadata": {},
   "source": [
    "This slope defines the rate of increase each year - here we estimate that on average during 2010-2020, the rate of increase was about 2.37 ppm/year.\n",
    "\n",
    "We can plot the model on an independent x-axis as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec837bb-55dc-44a3-b65c-546bdaa6990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model mx+b and apply it over the range of data values\n",
    "\n",
    "\n",
    "\n",
    "# recreate the plot with the line of best fit\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x,y,'k.',label='data')\n",
    "plt.plot(model_x, model_y,label='model')\n",
    "plt.xlabel('x (year)')\n",
    "plt.ylabel('y (CO$_2$ concentration, ppm)')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9baac5e-7c41-47ab-a4d0-3a0bc3f976ce",
   "metadata": {},
   "source": [
    "### The mechanics of linear regression\n",
    "\n",
    "Neat - we can pretty easily fit a line to some data. But what happened under the hood?\n",
    "\n",
    "In this process, `numpy` has minimized the error between the points and the line to find the best estimates of the slope $m$ and the slope $b$. In most math textbooks, the error between the data and model is referred to as a \"cost function\" and, in this case, is given as\n",
    "\n",
    "$$\n",
    " J = \\frac{1}{N} \\sum_{i=1}^N (y_{data,i} - y_{model,i})^2\n",
    "$$\n",
    "\n",
    "This formula is called the **Mean Squared Error** (MSE) since, well, it is the mean of the errors squared.\n",
    "\n",
    "We can code this up as function using `numpy` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b68e0-2883-472c-8930-8a57063debc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the cost function here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82d074-249e-46dc-8a8a-69d24e9eaff4",
   "metadata": {},
   "source": [
    "Using the above cost function, we can compute and visualize the \"error space\" -- the cost between data and model for a range of the parameters $m$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f0463-f31c-429f-aa01-eccd14ddfe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_space = np.linspace(-20,20,200)\n",
    "slope_space = np.linspace(0,5,200)\n",
    "I, S = np.meshgrid(intercept_space, slope_space)\n",
    "Error = np.zeros((200,200))\n",
    "\n",
    "# fill in the error matrix\n",
    "for row in range(np.shape(I)[0]):\n",
    "    for col in range(np.shape(S)[1]):\n",
    "        Error[row,col] = cost_function(y, I[row,col]+S[row,col]*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d223a4-7fea-4b4d-9a6e-3e107cedbb40",
   "metadata": {},
   "source": [
    "Then, we can visualize our best estimate on this error space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca41ee-bc31-4773-a50b-8b80c8b956a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = plt.pcolormesh(intercept_space,slope_space, np.log10(np.sqrt(Error)))\n",
    "plt.contour(intercept_space,slope_space, np.log10(np.sqrt(Error)),colors='white',linewidths=0.7)\n",
    "plt.plot(b, m, 'wo')\n",
    "plt.text(b+2, m, '$\\\\leftarrow$ Best Fit Parameters',color='white',va='center')\n",
    "plt.colorbar(C, label='log(cost)')\n",
    "plt.xlabel('intercept ($b$)')\n",
    "plt.ylabel('slope ($m$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e745792-4506-4b67-866d-4695cdd1ad1a",
   "metadata": {},
   "source": [
    "As we can see, out best fit parameters are those which minimize the error. But how does this work?\n",
    "\n",
    "### Computing the minimum of the cost function\n",
    "\n",
    "In the above example, we visualized the error for a relatively large range of slope and intercept values - we might refer to this as the \"brute force\" method. This is possible because we have a very simple model and a simple cost function. \n",
    "\n",
    "To see how the solution is computed, first we need to consider how the problem could be formulated as a matrix set of equations. Note that in this process, we are looking for values $b$ and $m$ such that the difference between model values $\\hat{y}$ and data values $y$ are minimimized. The model values are given by the following set of equations for our $N$ input data values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_1 & = m x_1 + b \\\\ \n",
    "\\hat{y}_2 & = m x_2 + b \\\\ \n",
    "&\\vdots \\\\\n",
    "\\hat{y}_N & = m x_N + b \\\\ \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can organize in a vector format as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N \\end{bmatrix} = m \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} + b\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "Using a boldface notation for vectors, we might also write this as\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{y}} = m \\textbf{x} + b\n",
    "$$\n",
    "\n",
    "However, we can also go one step further to utilize the dot product notation:\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{y}} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N \\end{bmatrix} = m \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} + b\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots\\\\ 1 & x_N \\end{bmatrix} \\cdot \\begin{bmatrix} b \\\\ m \\end{bmatrix} = \\textbf{X} \\cdot \\textbf{w}\n",
    "$$\n",
    "\n",
    "where here we have defined a matrix **X** which has a column of ones corresponding to the slope value $b$ and another column with the data values, and our model is now simply denoted as\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{y}} = \\textbf{X} \\cdot \\textbf{w}\n",
    "$$\n",
    "\n",
    "Given, this system of equations, it can be shown (using a few derivatives) that the following value of **w** results in a minimum of the MSE cost function $J$ defined above:\n",
    "\n",
    "$$\n",
    "\\textbf{w} = (X^TX)^{-1}\\textbf{X}^T\\hat{\\textbf{y}} \n",
    "$$\n",
    "\n",
    "Here, $^T$ denotes matrix transpose and $^{-1}$ indicates matrix inversion. I am not showing the derivation here, but we can try this out to ensure that the results match (they should since this is essentially what numpy is doing!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a599576-ea9c-46dd-8132-7fd6e8c00095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X\n",
    "\n",
    "\n",
    "# ensure y is a column vector\n",
    "\n",
    "\n",
    "# run the computation above\n",
    "\n",
    "\n",
    "# print the estimated slope and intercept values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eaf284-a7fa-436b-bf1b-620c5e4ccaee",
   "metadata": {},
   "source": [
    "Phew, looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979fa93-b84f-4812-816b-39fa53393c4a",
   "metadata": {},
   "source": [
    "## Linear Regression as an Iterative Process\n",
    "\n",
    "The above problem is very simple and it is unique in that it can be solved pretty straight-forwardly with pen, paper, some calculus, and a bit of patience. However, that's not always the case. One alternative is to approach this as an iterative process instead of a deterministic process. In other words, if we have a guess at a set of parameters (the slope and the intercept), we can use the geometry of the error space to determine the right set of parameters by making updates to our guess?\n",
    "\n",
    "To facilitate this process, we \n",
    "\n",
    "$$\n",
    " J = \\frac{1}{N} \\sum_{i=1}^N (y_{data,i} - y_{model,i})^2 = \\frac{1}{N} \\sum_{i=1}^N (y_{data,i} - (mx_i+b))^2\n",
    "$$\n",
    "\n",
    "are confronted with a \n",
    "need to compute the gradient of our cost function.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial b} & = \\\\\n",
    "\\frac{\\partial J}{\\partial m} & = \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc987d-712c-48de-8fa0-d9a2c09fc90a",
   "metadata": {},
   "source": [
    "After we've computed the cost, we code it up in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa329dc-c056-4d58-b182-95940356a8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f146430-a02c-4975-b259-acb167c3765c",
   "metadata": {},
   "source": [
    "To use our cost function gradient, let's make an initial guess and define a learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffa5ec-ba4c-4c3d-a688-5c49e1a6664f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d79f5d6-6f4b-4f37-8a22-08af5b77e34d",
   "metadata": {},
   "source": [
    "After one iteration, we can update our initial guess as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12abe34-3568-4159-8563-411d35711ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "371de9f1-e62e-4320-a2a8-1aab0f37210c",
   "metadata": {},
   "source": [
    "We can imagine doing this over and over again until we converge on a best estimate. Let's build a slider to examine how this would look over many iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d35562-1643-4fe1-a07c-cbf78db8f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_and_cost(initial_guess, n_iterations):\n",
    "\n",
    "    weights = np.copy(initial_guess)\n",
    "    for n in range(n_iterations):\n",
    "        y_modeled = weights[0]+weights[1]*x\n",
    "        weight_gradient = cost_function_gradient(x, y, y_modeled)\n",
    "        weights -= learning_rate*weight_gradient\n",
    "    \n",
    "    fig = plt.figure(figsize=(11,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(x, y,'k.')\n",
    "    plot_x = np.linspace(np.min(x), np.max(x), 100)\n",
    "    modeled_y = weights[0] + weights[1]*plot_x\n",
    "    plt.plot(plot_x, modeled_y,'b-',\n",
    "            label='Fit: '+'{:.2f}'.format(weights[1])+'*x + ' +'{:.2f}'.format(weights[0]))\n",
    "    plt.plot(plot_x, b + m*plot_x,'b--',\n",
    "             label='Best Fit: '+'{:.2f}'.format(m)+'*x + ' +'{:.2f}'.format(b))\n",
    "    plt.title('Fit after '+str(n_iterations)+' iteration(s)')\n",
    "    plt.legend(loc=2)\n",
    "    plt.xlim([-1,11])\n",
    "    plt.ylim([-5,30])\n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('x')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    C = plt.pcolormesh(intercept_space,slope_space, np.log10(Error))\n",
    "    plt.contour(intercept_space,slope_space, np.log10(Error),colors='white',linewidths=0.7)\n",
    "    plt.plot(initial_guess[0], initial_guess[1], 'wo')\n",
    "    plt.plot(weights[0], weights[1], 'wo')\n",
    "    plt.text(initial_guess[0]+2, initial_guess[1], '$\\\\leftarrow$ Initial',color='white',va='center')\n",
    "    if n_iterations>0:\n",
    "        plt.text(weights[0]+2, weights[1], '$\\\\leftarrow$ Final',color='white',va='center')\n",
    "    plt.colorbar(C, label='log(cost)')\n",
    "    plt.title('Error space')\n",
    "    plt.ylabel('slope ($m$)')\n",
    "    plt.xlabel('intercept ($b$)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca3852-1da7-442a-a0bf-5e83be5b6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_fit_and_cost, initial_guess=fixed(np.array([intercept, slope])),\n",
    "         n_iterations=widgets.IntSlider(min=0, max=1000));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce832e46-efcc-40f8-b205-f42d0206263c",
   "metadata": {},
   "source": [
    "In the above demonstration, we have \"learned\" the right parameters for our model by \"training\" it on our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b571f8",
   "metadata": {},
   "source": [
    "**Key Takeaways**\n",
    "\n",
    "1. A linear model can be expressed in matrix form as \n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{y}} = \\textbf{X} \\cdot \\textbf{w}\n",
    "$$\n",
    "\n",
    "2. The optimum parameters for the model (**w**) are those which minimize a cost function - the metric for how the model predictions differ from the data.\n",
    "\n",
    "3. By considering the gradient of our cost function relative to the model parameters, we can solve for the parameters iteratively by slowly taking steps toward a minimum in the cost function. This process is called *gradient descent*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
