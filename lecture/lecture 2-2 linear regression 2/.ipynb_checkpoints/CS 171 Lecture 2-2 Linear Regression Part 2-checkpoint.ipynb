{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa78574b-d9db-4449-b800-0aa9b1460993",
   "metadata": {},
   "source": [
    "# Implementing Linear Regression Model as a Python Class\n",
    "In this notebook, we will revisit linear regression and consider implementing it as a trainable machine learning model.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this notebook, you should be able to\n",
    "\n",
    "1. List machine learning key terms and identify how they differ from notation in traditional math textbooks.\n",
    "2. Write a Python class to implement linear regression.\n",
    "3. Visualize the loss function of a model as a function of the training iterations.\n",
    "\n",
    "**Import modules**\n",
    "Begin by importing the modules to be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5b778-fdba-4ce4-b322-aa326262b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce832e46-efcc-40f8-b205-f42d0206263c",
   "metadata": {},
   "source": [
    "In the previous notebook, we saw how we could fit a linear line to some data using an iterative approach. In this notebook, we are going to continue with this theme, abstracting some of the key ideas into a structure and approach we can use for a wide variety of problems.\n",
    "\n",
    "## Formulating a Model Framework\n",
    "\n",
    "To begin, let's reconsider our linear regression model and introduce some new terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3861b1-c0a1-488f-9a2c-9aad7ddd4053",
   "metadata": {},
   "source": [
    "In our math classes, we describe the line we are trying to fit to the data as having a slope $w$ (err, perhaps $m$) and intercept $b$. We combine the these constants together in line with the equation\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "where the \"hat\" symbol above the $y$ indicates it is a model estimate of a true value $y$. In the world of Machine Learning, we will adopt some new terminology.\n",
    "\n",
    "### The Forward Model\n",
    "The first update to our terminology is that we will start by calling $w$ a **weight** and $b$ a **bias**. These *parameters* are utilized by our model to produce a *predicted value*. This part of the line-fitting process is what we'll call the **Forward Model**.\n",
    "\n",
    "In our previous notebook, we saw that would could write our equations in matrix form as \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} = w \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} + b\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "or, more succinctly as\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{y}} = \\textbf{X} \\cdot \\textbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc16d4d-85da-41b0-9f3c-6b0dff1137ea",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Once we have our predicted values, we can compare with our target values to determine our error. We compute these differences using a **Loss Function** (which in math class and our previous notebook we have referred to as a \"cost function\" or simply an error). For linear regression, our Loss Function is the *Mean Square Error*\n",
    "\n",
    "$$\n",
    "L =\\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Based on these differences, we can then update our weight and biases to produce predicted values which are closer to our target values. This step is the **Model Training** step and utilizes the idea of gradient descent. In our previous notebook, we computed the gradients of our cost function relative to both the weight and the bias:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b} & = \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i))\\\\\n",
    "\\frac{\\partial L}{\\partial w} & = \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)x_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can simplify this in notation considerably again using vector notation. Let's collect the two partial derivatives into vector derivative with\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\textbf{w}} &= \\begin{bmatrix} \\frac{\\partial L}{\\partial b} & \\frac{\\partial L}{\\partial w} \\end{bmatrix} \\\\\n",
    "&= \\frac{-2}{N}  \\begin{bmatrix} \\sum_{i=1}^N (y_i - \\hat{y}_i)) & \\sum_{i=1}^N (y_i - \\hat{y}_i)x_i \\end{bmatrix} \\\\\n",
    "&= \\frac{-2}{N} \\begin{bmatrix} (y_1 - \\hat{y}_1) (y_2 - \\hat{y}_2) \\cdots (y_N - \\hat{y}_N) \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots\\\\ 1 & x_N \\end{bmatrix}\\\\\n",
    "&= \\frac{-2}{N} (\\textbf{y} - \\hat{\\textbf{y}})^T \\cdot \\textbf{X}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Or  more concisely:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{w}} = \\frac{-2}{N} (\\textbf{y} - \\hat{\\textbf{y}})^T \\cdot \\textbf{X}\n",
    "$$\n",
    "\n",
    "This notation will be particularly advantageous when we code up our solution below.\n",
    "\n",
    "Here we have taken our simple example and given it a bit of a structure that will allow us to apply it to a large range of applications. This process - the construction of a forward model and the definition of the loss function - is at the heart of many approaches in Machine Learning algorithms.\n",
    "\n",
    "We can put these steps together into a diagram as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c955e0-4ce6-46cb-94ff-59d7be5eee2c",
   "metadata": {},
   "source": [
    "<img src=\"Linear_Regression_Schematic.png\" alt=\"Linear Regression Schematic\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea66431-df40-4e50-956d-369cfd53aa58",
   "metadata": {},
   "source": [
    "## Defining a Linear Regression Python Class\n",
    "\n",
    "Now that we've got some notation out of the way, let's degine a Python structure that utilizes our generalized notation. We will do this in the form of a `class` which we use to make objects. Many of the existing Machine Learning packages use this approach, so we will follow with this paradigm. Let's check out out example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3a91b-d776-4b53-8fdc-8ccf3a976c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear regression model for predicting continuous outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, learning_rate, n_iterations, random_seed=1):\n",
    "        \"\"\"\n",
    "        Initializes the LinearRegression model and its parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Input feature matrix used to determine the number of weights.\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent.\n",
    "        n_iterations : int\n",
    "            Number of iterations for training.\n",
    "        random_seed : int, optional\n",
    "            Random seed for reproducibility (default is 1).\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def initialize(self, X):\n",
    "        \"\"\"\n",
    "        Initializes the weight vector with small random values.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_seed)\n",
    "        self.w = np.random.normal(loc = 0.0, scale = 0.1, size = np.shape(X)[1])\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Computes the predicted output using the current weights.\n",
    "        \"\"\"\n",
    "        y_hat = X @ self.w\n",
    "        return(y_hat)\n",
    "\n",
    "    def loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Computes the mean sum of squared errors loss.\n",
    "        \"\"\"\n",
    "        return (1 / np.size(y)) * np.sum((y-y_hat)**2)\n",
    "\n",
    "    def loss_gradient(self, X, y, y_hat):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss with respect to weights.\n",
    "        \"\"\"\n",
    "        return (-2 / np.size(y)) * (y - y_hat).T @ X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the linear regression model using gradient descent.\n",
    "        \"\"\"\n",
    "        for iteration in range(self.n_iterations):\n",
    "            y_hat = self.forward(X)\n",
    "            gradient = self.loss_gradient(X, y, y_hat)\n",
    "            self.w -= gradient * self.learning_rate\n",
    "            self.losses.append(self.loss(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec3d21-57cd-454a-a9d4-913209f67c37",
   "metadata": {},
   "source": [
    "### Using a Model Object\n",
    "\n",
    "Just as in our first notebook, let's take a look at some data. Here, we'll use the same dataset for carbon dioxide concentration as we did previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3af1e-5ef2-435d-a2bc-a8a473985bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset as shown in the previous notebook\n",
    "df = pd.read_csv('monthly_in_situ_co2_mlo.csv', skiprows=64)\n",
    "df = df[df.iloc[:, 4] >0]\n",
    "\n",
    "# subset to a given time range\n",
    "df_subset = df[(df.iloc[:,3]>=2010) & (df.iloc[:,3]<=2020)]\n",
    "\n",
    "# redefine x and y\n",
    "x = np.array(df_subset.iloc[:,3])\n",
    "y = np.array(df_subset.iloc[:,4])\n",
    "\n",
    "# remove the first value of y\n",
    "x = x-2010\n",
    "y = y-y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e6403",
   "metadata": {},
   "source": [
    "As a reminder, these points were historically collected using individual measurements! The following picture shows Charles Keeling taking one such measurement:\n",
    "\n",
    "<img src=\"Keeling_CO2_Measurement.jpg\" alt=\"Charles Keeling taking a CO2 measurement\" width=\"500\">\n",
    "\n",
    "Photo Credit: Keeling Family, [Scripps Institute of Oceanography](https://scrippsco2.ucsd.edu/history_legacy/charles_david_keeling_biography.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed698a6-1b44-4231-8114-ff5b30772a26",
   "metadata": {},
   "source": [
    "Using the notation defined above, let's make our **X** matrix using our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963cd8e-96a0-4d98-b03c-6c34bb885050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f74ab-543b-4e45-9563-89cf36169b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a0b15-cb34-4f3d-9a44-a7384873dce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03426b54-e0da-4d64-9e5b-cba2b1e0aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k.',label='$y_{target}$')\n",
    "plt.plot(x,y_predict,'b-',label='$y_{predicted}$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c87a0-e910-4af7-beb2-d3d3b0adeaf1",
   "metadata": {},
   "source": [
    "As we can see, our model isn't good at all - and that's because we haven't trained it on our data yet. Let's check the weight and the bias of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32deaf3-bdd1-4092-b905-d07f9e8d29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bias:',model.w[0])\n",
    "print('Weight:',model.w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fbe6a-03a6-4897-b57f-f09458825f76",
   "metadata": {},
   "source": [
    "These values were simply generated with the random value generator in the `initialize` function. We use random values for initializing the bias and weight rather than 0 so that the gradients can be computed.\n",
    "\n",
    "Let's go ahead and train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc1f54-d53e-4fb7-95da-33b873af3eeb",
   "metadata": {},
   "source": [
    "### Training a Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7041c802-1e7e-4544-8ac2-39d9a77989df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9d5a8-113a-4e93-aaef-83f51743f8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45995d-9e41-4f83-aef2-e430ff2d04ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408229b-ba54-4fea-b610-414a1277c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k.',label='$y_{target}$')\n",
    "plt.plot(x,y_predict,label='$y_{predicted}$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04863232-8b0d-4307-9420-16ca7c086897",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ca936",
   "metadata": {},
   "source": [
    "**Key Takeaways**\n",
    "\n",
    "1. The loss function gradients with respect to model parameters can be expressed in vector notation as \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{w}} = \\frac{-2}{N} (\\textbf{y} - \\hat{\\textbf{y}})^T \\cdot \\textbf{X}\n",
    "$$\n",
    "\n",
    "2. The iterative solution of model parameters can be built into a Python class with a series of functions that initialize and fit the model to data.\n",
    "\n",
    "3. The model loss function provides key information on the convergence of the model parameters to the \"best guess\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
