{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa78574b-d9db-4449-b800-0aa9b1460993",
   "metadata": {},
   "source": [
    "# An Introduction to PyTorch\n",
    "In this notebook, we will get familiar with PyTorch, a powerful machine learning code base designed for neural networks and other machine learning applications.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Describe the PyTorch package and why it is faster than implementing models in NumPy from scratch.\n",
    "2. Outline the components of a neural network implemented in PyTorch.\n",
    "3. Implement a multi-layer perceptron in PyTorch.\n",
    "\n",
    "**Import modules**\n",
    "\n",
    "Begin by importing the modules to be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc5b778-fdba-4ce4-b322-aa326262b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16fa322e-368f-436c-b159-582f7a53cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules from PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ac93d-783e-4f5f-92ce-75de96717fcd",
   "metadata": {},
   "source": [
    "## What is PyTorch?\n",
    "\n",
    "PyTorch is a machine learning framework that bundles up all of the calculus and linear algebra necessary to build and train neural networks behind the scenes, allowing the user to focus on the structure of the network. The calculations are done with PyTorch *tensors* which are similar to `numpy` arrays but designed to leverage the power of GPUs when present (although the functions also work on CPUs). When using PyTorch, we can check whether we have a GPU available on our machine and use this for calculations. Otherwise, we can just use CPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6bee836-5ea1-4db2-955c-6f692d89441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\"\n",
    "print('Using device: '+device_name)\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11adfee3-345f-4284-a52e-ee117fd6c426",
   "metadata": {},
   "source": [
    "With our device identified, we are ready to start some calculations. Just like with `numpy`, we can perform basic matrix calculations with PyTorch tensors. Let's check on the speed of these calculations using a matrix multiplication example. Begin by defining some random `numpy` arrays, and then an identical tensor in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a76059e-068b-4846-8dd4-76ec903ea3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a matrix size\n",
    "n = 100\n",
    "\n",
    "# define numpy matrices \n",
    "A = np.random.randn(n, n).astype(np.float32)\n",
    "B = np.random.randn(n, n).astype(np.float32)\n",
    "\n",
    "# define equivalent PyTorch tensors\n",
    "A_tensor = torch.tensor(A, device = device_name).float()\n",
    "B_tensor = torch.tensor(B, device = device_name).float()\n",
    "\n",
    "# define the number of times to repeat the calculation\n",
    "repeats = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5213e6cc-9dda-40c5-bbd2-7680703f1d2f",
   "metadata": {},
   "source": [
    "With these matrices in hand, we can test the timing of matrix multiplication. First let's check with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e20729ab-a5d3-44a0-8a13-c602ec19be80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average numpy matrix multiplication time: 0.0439 ms\n"
     ]
    }
   ],
   "source": [
    "# numpy check\n",
    "C = A @ B # dummy calculation\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(repeats):\n",
    "    C = A @ B\n",
    "end = time.perf_counter()\n",
    "\n",
    "average_time = 1000*(end - start) / repeats\n",
    "print('Average numpy matrix multiplication time: '+'{:.4f}'.format(average_time)+' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f51cf92-ddfc-4a03-8cee-d07e436a02b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tensor matrix multiplication time: 0.0485 ms\n"
     ]
    }
   ],
   "source": [
    "# tensor check\n",
    "C_tensor = A_tensor @ B_tensor # dummy calculation\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(repeats):\n",
    "    C_tensor = A_tensor @ B_tensor\n",
    "end = time.perf_counter()\n",
    "\n",
    "average_time = 1000*(end - start) / repeats\n",
    "print('Average tensor matrix multiplication time: '+'{:.4f}'.format(average_time)+' ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244554a-dd71-4ad0-b039-0cd10a70048f",
   "metadata": {},
   "source": [
    "Note: the time difference between these calculations will become apparent after running the calculation several times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f34d2-a5e2-45aa-a931-3aff21a69290",
   "metadata": {},
   "source": [
    "## The MNIST Dataset\n",
    "In this notebook, we will again use the MNIST hand-drawn image data set as an example to test out models in PyTorch. Let's re-create the function to read in the data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4721c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist_images(data_directory, subset='train'):\n",
    "    if subset=='train':\n",
    "        prefix = 'train-'\n",
    "    else:\n",
    "        prefix = 't10k-'\n",
    "        \n",
    "    with open(os.path.join('MNIST',prefix+'images.idx3-ubyte'), 'rb') as f:\n",
    "        # unpack header\n",
    "        _, num_images, num_rows, num_cols = struct.unpack('>IIII', f.read(16))\n",
    "        \n",
    "        # read image data\n",
    "        image_data = f.read(num_images * num_rows * num_cols)\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8)\n",
    "        images = images.reshape(num_images, num_rows, num_cols)\n",
    "\n",
    "    with open(os.path.join('MNIST',prefix+'labels.idx1-ubyte'), 'rb') as f:\n",
    "        # unpack header\n",
    "        _, num_labels = struct.unpack('>II', f.read(8))\n",
    "\n",
    "        # read label data\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595de75-c811-4549-a039-6a4c60658a88",
   "metadata": {},
   "source": [
    "Next, let's read in the data and reshape into input arrays as in previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5bd958e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the training and test images\n",
    "train_images, train_labels = read_mnist_images('MNIST','train')\n",
    "test_images, test_labels = read_mnist_images('MNIST','test')\n",
    "\n",
    "# reshape as before\n",
    "X_train = train_images.reshape(-1, 784) / 255.0\n",
    "X_test = test_images.reshape(-1, 784) / 255.0\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5d288-6e68-4577-974b-ff75a0ab13e0",
   "metadata": {},
   "source": [
    "In this implementation, we have read in our data into `numpy` arrays. However, if we want to leverage the power of PyTorch, we'll need to change these arrays in PyTorch tensors. Let's do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23476d91-9d31-46b1-b939-8dde9fbdce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PyTorch tensor versions of the MNIST data\n",
    "X_train_tensor = torch.tensor(X_train, device = device_name).float()\n",
    "X_test_tensor  = torch.tensor(X_test , device = device_name).float()\n",
    "y_train_tensor = torch.tensor(y_train, device = device_name).float()\n",
    "y_test_tensor  = torch.tensor(y_test , device = device_name).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248bdf1-2489-4cb5-ae97-c4189a41153b",
   "metadata": {},
   "source": [
    "In addition, since we're going to implement our model with a one-hot encoding, let's leverage PyTorch's `one_hot` function to make equivalent arrays for our image labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ef6331c-b35b-400d-8353-91b6a1165deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one-hot encoded training label tensors\n",
    "y_train_onehot = F.one_hot(y_train_tensor.to(torch.int64), num_classes = 10).float()\n",
    "y_test_onehot  = F.one_hot(y_test_tensor .to(torch.int64), num_classes = 10).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b7357-a614-47b9-acca-c267704c4e7f",
   "metadata": {},
   "source": [
    "## The Single Layer Perception Revised: A PyTorch Implementation\n",
    "\n",
    "In our previous lesson, we saw an implementation of the single layer perceptron with a single hidden layer that we wrote as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a9d45d-79df-446b-8bc4-af3eaddc9884",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "z &= Xw^T+b \\,\\,\\,&\\text{   (Linear Layer)} \\\\ \n",
    "p &= \\sigma(z)  \\,\\,\\,&\\text{   (Activation Function)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c379700-88f3-49b2-851e-2c98bb03584e",
   "metadata": {},
   "source": [
    "Let's see how we implement this in PyTorch using a `SingleLayerPerceptron` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c3304134-3e22-4935-b88d-b1d7d0f97c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLP(nn.Module):\n",
    "\n",
    "    # define an init function with a Linear layer\n",
    "    def __init__(self, in_dim = 784, out_dim = 10):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    # implement the forward method\n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51622c2f-7735-441f-beba-5bdf55702063",
   "metadata": {},
   "source": [
    "Here, we can see that we made a subclass of the `nn.Module` class - a highly flexible framework for building neural networks. We can also see that we've created our linear layer using the `nn.Linear` class and we've called the `sigmoid` function, which is already provided by PyTorch - convenient! These simple calls will handle all of the matrix multiplation behind the scenes, and we just need to provide the shapes of our data.\n",
    "\n",
    "Once we have our class, we can make our model object, just like we've done with our previous classes. The only difference is that we implement the model with our computational resources in mind (GPU vs CPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c370722-f84b-47ec-92a8-c667b7d00c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an slp model object here\n",
    "slp = SLP().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a1753-37d5-4ca3-ae21-8befbf9b9fba",
   "metadata": {},
   "source": [
    "Ok, now that we have our model, how do we train it? If we take a look above, we see that we did NOT implement a backward method - that's a little curious given that we spent so much time last lesson computing gradients of the loss function to optimize the weights. We'll see why we didn't implement this shortly. First, let's define some important features that will be use to train our model.\n",
    "\n",
    "#### The DataLoader\n",
    "\n",
    "We'll start with the `TensorDataset` and the `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5b74f639-fb89-4e7e-99cd-f012e0ece592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a TensorDataset object for the training data\n",
    "ds = TensorDataset(X_train_tensor, y_train_onehot)\n",
    "\n",
    "# make a loader object for the training data\n",
    "loader = DataLoader(ds, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cf3511e5-dd20-40eb-80d2-a3dd65f2cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a TensorDataset object for the test data\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_onehot)\n",
    "\n",
    "# make a loader object for the test data\n",
    "test_loader = DataLoader(test_ds, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8e47f-4eb7-4e50-97e8-fe468dcea242",
   "metadata": {},
   "source": [
    "These objects will allow us to efficiently train and use our model data. The `DataLoader` is a object that will provide subsets of our data from the dataset at a size given by `batch_size`. This will allow the model to be trained on mini-batches of data, rather than on the entire dataset at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a20ffc-027c-4013-b28a-ff20f94f1861",
   "metadata": {},
   "source": [
    "#### The Loss Function\n",
    "The definition of the loss function is key to the model training. Many of the common loss functions are are built into PyTorch (although you can also define your own loss function, if desired). Following the previous example, let's use the mean square error loss function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1aa62e3a-2f12-4619-96d7-d2766920ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a criterion as the Mean Square Error Loss Function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a288a51-7bd5-4b75-9938-c74eb3c2d596",
   "metadata": {},
   "source": [
    "This loss function will take in the output of the forward model as well as the training labels provided by the `DataLoader` above. Then, **the loss of the model computed from this function will be used by PyTorch to compute the gradients of the loss function with respect to the weights!** This is a key functionality of PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4300e7-c2de-4a66-b6e4-4d12a66f8e21",
   "metadata": {},
   "source": [
    "#### Updating the Weights\n",
    "To implement the update to the weights copmuted with the gradients of the loss function, we need to implement a gradient decent algorithm. Let's define that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8be378e6-0253-4104-a3e7-72cd4645b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a gradient descent algorithm\n",
    "optimizer = torch.optim.SGD(slp.parameters(), lr = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f429e45-fccb-4a9f-93de-8a01831bfc56",
   "metadata": {},
   "source": [
    "In the above code block, `SGD` stands for *Stochastic Gradient Descent* and it implements the gradient descent algorithm for us (i.e. it will update the weights in our model based on the gradients of our loss function). It is called *Stochastic* because the gradients are not computed on the entire dataset the way we implemented them in our previous perceptron example. Instead, the gradients are computed on a random sample of the data as determined by the `DataLoader`. The `SGD` algorithm is one possible gradient decesnt algorithm but we will see more examples in future applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4973426-b1d5-4192-85c5-8cb90c79a93c",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now that we've got our model, our loss function, and our gradient decent algorithm, we're ready to train our model. Just as before, let's run our forward, backward, and gradient decent algorithms, keeping track of the training and testing losses/accuracies as we go: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ec32a43-76c8-4945-80c5-66e7b178265d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m loss.backward()\n\u001b[32m     26\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m loss.backward()\n\u001b[32m     28\u001b[39m optimizer.step()\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# compute the total loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m torch.autograd.backward(\n\u001b[32m    582\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    583\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m _engine_run_backward(\n\u001b[32m    348\u001b[39m     tensors,\n\u001b[32m    349\u001b[39m     grad_tensors_,\n\u001b[32m    350\u001b[39m     retain_graph,\n\u001b[32m    351\u001b[39m     create_graph,\n\u001b[32m    352\u001b[39m     inputs,\n\u001b[32m    353\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    354\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    355\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\cs171\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    827\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# make empty arrays to store the losses for each epoch\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "# loop through each epoch\n",
    "for epoch in range(10):\n",
    "\n",
    "    #- Training Block --------------------------------\n",
    "    slp.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    \n",
    "    # loop through mini-batches in the data loader\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # forward\n",
    "        p = slp.forward(x)\n",
    "        \n",
    "        # backward\n",
    "        loss = criterion(p, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute the total loss\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "        # predictions (argmax across 10 classes)\n",
    "        pred = p.argmax(dim = 1)\n",
    "        true = y.argmax(dim = 1)\n",
    "        correct += (pred == true).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    train_loss = total_loss/total\n",
    "    train_acc = correct/total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    print(f\"epoch {epoch+1}: loss={total_loss/total:.4f}, acc={correct/total:.3f}\")\n",
    "\n",
    "    # ---- Evaluation (Testing/Validation) ----\n",
    "    slp.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            p = slp(x)\n",
    "            loss = criterion(p, y)\n",
    "       \n",
    "            # compute the total loss\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "            # predictions (argmax across 10 classes)\n",
    "            pred = p.argmax(dim = 1)\n",
    "            true = y.argmax(dim = 1)\n",
    "            correct += (pred == true).sum().item()\n",
    "            total += x.size(0)\n",
    "            \n",
    "\n",
    "    test_loss = total_loss/total\n",
    "    test_acc = correct/total\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ae8a1-e656-4da7-af28-cd773c2e5901",
   "metadata": {},
   "source": [
    "The model seems to be doing ok based on the reported losses. Let's have a look at the losses for the training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae18111-0851-4dfb-b116-c73e96a41d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses,label='Train')\n",
    "plt.plot(test_losses,label='Test')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs,label='Train')\n",
    "plt.plot(test_accs,label='Test')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a05d2-aa32-4a47-955c-b96779de8f0e",
   "metadata": {},
   "source": [
    "Let's see how our model does with the classification of our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc30e5b-7ef0-47a7-967e-1ac1d63e8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose some indices of the images to test\n",
    "np.random.seed(37)\n",
    "test_indices = np.random.randint(low=0, high=10000, size=10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for d, index in enumerate(test_indices):\n",
    "\n",
    "    # convert the image corresponding to this index to a tensor and predict\n",
    "    # the label with the model\n",
    "    \n",
    "\n",
    "    # show the image with the predicted label\n",
    "    plt.subplot(2,5,d+1)\n",
    "    plt.imshow(train_images[index,:,:],cmap='Greys')\n",
    "    plt.title('True Label: '+str(train_labels[index])+\\\n",
    "              '\\n Predicted Label: '+str(predicted_label))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cb866-3c95-4c95-8948-0c0dfaa1ef63",
   "metadata": {},
   "source": [
    "## Try it for yourself!\n",
    "\n",
    "After implementing our single layer perceptron, we added a new layer to make our multilayer perceptron. We wrote this in equations as follows: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\zeta &= Xw_1^T+b_1 \\,\\,\\,&\\text{   (First Linear Layer)}\\\\ \n",
    "h &= \\sigma(\\zeta) \\,\\,\\,&\\text{   (First Activation Function)} \\\\ \n",
    "z &= hw_2^T+b_2 \\,\\,\\,&\\text{   (Second Linear Layer)} \\\\ \n",
    "p &= \\sigma(z)  \\,\\,\\,&\\text{   (Second Activation Function)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c3332-cad6-49fb-b1df-6402a22d27cd",
   "metadata": {},
   "source": [
    "Following the structure above, implement a `MLP` class here. You should only need to make a few small additions to the `SLP` class above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e9f08-f409-4491-8c96-b12d60888f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement an MLP class here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09bba9-4f47-402a-9503-e04d10e18d14",
   "metadata": {},
   "source": [
    "Next, create an object with your class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb39ad9-7441-46d1-82da-252f2a3e9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an mlp object here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb40cfd-356f-4ab5-b94c-b73322a899d9",
   "metadata": {},
   "source": [
    "Then, define a loss function and a gradient decent algorithm for your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696d4ed-1aa3-48a6-ada7-85ed1c85e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the criterion and optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e21da9-20db-43cb-afdf-f0d3da502d0c",
   "metadata": {},
   "source": [
    "Now, your model is ready to train! Implement a training loop here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4a724-a8ae-4484-80d5-83b34e8dc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a training and testing loop here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae39406-4272-4a48-ae17-69f920b73a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c0269f-e2a4-4fdc-8669-e2520ade9f33",
   "metadata": {},
   "source": [
    "Compare and contrast the single and multi-layer models creaed above. Which is the \"better\" model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a3214-b13c-407f-bfb0-1b53d27ea7f8",
   "metadata": {},
   "source": [
    "*Modify this markdown cell with your notes*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
