{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f7750d-7994-4c20-8f0f-5199844a90fe",
   "metadata": {},
   "source": [
    "# Improving the Performance of Convolutional Neural Networks\n",
    "In this notebook, we'll examine ways to improve the performance of a Convolutional Neural Network. This notebook will build on the classification example shown in the previous notebook.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Implement a probabalitic drop-out to deduce the dependence of a network on any given weight or collection of weights.\n",
    "2. Use image transformations to augment the data in a training set.\n",
    "3. Test different strategies to improve the performance of a convolutional neural network.\n",
    "\n",
    "**Import modules**\n",
    "\n",
    "Begin by importing the modules to be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a271535-6afc-4923-bfa8-ce20edf4c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a1cb0-0ac7-47de-97cb-5ec791152a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d560f-b058-4ae4-b586-ee2a6ca365f0",
   "metadata": {},
   "source": [
    "Let's prepare our device to use PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6d6ef-46d5-4a9d-8c22-5bd00c838c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5bfc1-8425-48f2-82aa-2d048b946462",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "In our previous example, we built a convolutional neural network for image classification but we found that our model was too complex for our data - and the model \"memorized\" the dataset. In this notebook, we will explore two methods that can help alleviate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf23100-d4c7-4a03-95bf-066196188bfd",
   "metadata": {},
   "source": [
    "Let's redefine a few things that we can re-use from the previous notebook here.\n",
    "\n",
    "First, let's define our data loaders for the training and the testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b1b13-7e70-480b-95d5-91cca0477790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training and testing mechanisms to load in images as datasets\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# define the transform \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 3 values for RGB\n",
    "])\n",
    "\n",
    "# first contruct the training set\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join('Images','train'), transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# then the testing set\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join('Images','test'), transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969b010-7d97-46c4-9821-2d59d4e5f5f7",
   "metadata": {},
   "source": [
    "Next, we'll define a function to bundle up our training loop, but first we'll need to re-define our helper functionto compute the correct number of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c73287b-0008-40f1-9f0b-335fdcde2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correct_labels(outputs, labels):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    _, predicted_values = torch.max(outputs, 1)\n",
    "    differences = predicted_values - labels\n",
    "    for i in range(len(differences)):\n",
    "        if differences[i]==0:\n",
    "            correct +=1\n",
    "        total += 1\n",
    "    return(correct, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5000ae3-d965-4278-bb3a-06f4f8f4ebd3",
   "metadata": {},
   "source": [
    "Ok, with than in hand, we can define a training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aedb3e-6fdb-488f-ba61-ad5907b9de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function bundles up the training/testing loops from the previous notebook\n",
    "def training_loop(model, optimizer, NUM_EPOCHS, train_loader, test_loader, printing=True):\n",
    "    # make empty lists to keep track of the training and testing losses\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # loop through each epoch to run the training loop\n",
    "    # and check the model with the training data\n",
    "    # keep track of both sets of losses as your go\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        \n",
    "        # Run the training loop\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_images = 0\n",
    "        for train_inputs, train_labels in train_loader:\n",
    "            train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_inputs)\n",
    "            train_correct, train_total = compute_correct_labels(outputs, train_labels)\n",
    "            total_train_correct += train_correct\n",
    "            total_train_images += train_total\n",
    "            loss = criterion(outputs, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Run the testing loop\n",
    "        # this is essentially the same as the training loop but\n",
    "        # without the optimizer and backward propagation\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        total_test_correct = 0\n",
    "        total_test_images = 0\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_labels in test_loader:\n",
    "                test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "                outputs = model(test_inputs)\n",
    "                test_correct, test_total = compute_correct_labels(outputs, test_labels)\n",
    "                total_test_correct += test_correct\n",
    "                total_test_images += test_total\n",
    "                loss = criterion(outputs, test_labels)\n",
    "                total_test_loss += loss.item()\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        if printing:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\"+\\\n",
    "                  f\" - Train Loss: {avg_train_loss:.4f}, \"+\\\n",
    "                  f\"Train Correct: {total_train_correct}/{total_train_images} \"+\\\n",
    "                  f\"- Test Loss: {avg_test_loss:.4f}, \"+\\\n",
    "                  f\"Test Correct: {total_test_correct}/{total_test_images} \")\n",
    "\n",
    "    return(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0467b-eb15-4e56-9c95-624e88ac9dc4",
   "metadata": {},
   "source": [
    "Finally, let's redefine the cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bf887-d0eb-4931-9469-6ca6b5eaefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss for use in classification problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29db7ad-137c-4466-b6b6-38c6bf01562d",
   "metadata": {},
   "source": [
    "The cross entropy loss function is common for classification tasks and is defined as follows:\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^C y_{n,c} \\log(p_{n,c})\n",
    "$$\n",
    "\n",
    "Intuitively, we can see that, for a given training example, the probabilities will be zero for each class except for the one with the true label (since they are one-hot encoded). When $y_{n,c}=1$, the loss will be very large when $p_{n,c}$ is close to zero (due to the minus sign in front), and then loss will be small when $p_{n,c}$ is close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5526b8-88fd-40ea-9144-097548c2495e",
   "metadata": {},
   "source": [
    "For comparison, we can see this is similar to the MSE loss function we used in out single- and multi-layer perceptrons.\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^C (y_{n,c}-p_{n,c})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc649d1-959e-49b1-a657-e15b99dd1be3",
   "metadata": {},
   "source": [
    "With all these pieces in hand, we're ready to explore some of our approaches to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c2476-4a42-40f8-8072-1f43d84cf2b7",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout refers to the process of \"dropping out\" certain nodes in the network at random by setting their weights to 0 in one pass through the forward model. You'll recall that our linear layers are implemented with fully-connected nodes meaning all input features to a layer are connected to all output nodes via an individual weight. Dropping out is typicaly done on the fully-connected layers in a CNN but it may also be done in the convolutional layers.\n",
    "\n",
    "Since this technique is quite common, it is built into `nn` module of PyTorch and we can slip this into our CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a7090-2dca-4b94-a1a5-78f9b9cdc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is identical to that used in the previous notebook\n",
    "# add a dropout layer in the fully connected layers of the notebook\n",
    "class ClassificationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationCNN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2) \n",
    "        )\n",
    "        \n",
    "        conv_output_size = 32 * (25) * (25)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(conv_output_size, 128), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            # add a dropout layer here\n",
    "            \n",
    "            nn.Linear(128, 5)\n",
    "        )\n",
    "\n",
    "    # define the forward step\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e647735-0689-4cd6-9c25-8adbc46789e4",
   "metadata": {},
   "source": [
    "With this change, we can procede to training our model using our training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383f183-4754-455b-8610-6fc7f719d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the model here in case cells are run \n",
    "# out of order for instructional purposes\n",
    "model = ClassificationCNN().to(device)\n",
    "\n",
    "# Adam optimizer for stochastic gradient descent\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# define the number of epochs\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "# call the training loop function we defined above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4bba3f-3f64-4d32-a2cf-190df957a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, 'd-', label='Training Loss')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), test_losses, 'd-', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251fe96-1054-4a95-bdaa-ab383531aaa4",
   "metadata": {},
   "source": [
    "Looking at the plot above, we can see that the drop-out procedure helped with our issue of overfitting! Now, we don't see an extreme increase in the test losses as the training is proceding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03ad54-be6c-4642-a487-26152378a07d",
   "metadata": {},
   "source": [
    "### The Adaptive Moment Estimation Algorithm\n",
    "\n",
    "In this lesson and the previous one, we've been using the following algorithm as our optimizer: `optim.Adam(model.parameters(), lr=0.001)`\n",
    "\n",
    "Here, `Adam` (short for Adaptive Moment Estimation) is a gradient descent algorithm much like the (Stochastic) Gradient Descent algorithm (`SGD`) we've used in previous lessons. The difference here is that the `Adam` algorithm choose updates to weights in a \"smarter way\" than randomly taking steps toward the minimum of the loss function. This is done by computing the running mean and the variance of the gradients ss they are computed on each iteration. These quantities (or \"moments\") are used is used to scale the learning rate so that the steps are appropriately sized - e.g. large steps are taken when possible, but small steps are used there may be an issue of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202dc60-b3b2-440f-a061-07f3b5e8e6ee",
   "metadata": {},
   "source": [
    "### A peek at model performance on the validation set\n",
    "\n",
    "Let's take a peek at how things are looking in our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603c5e6-6106-4c83-b0ff-07979abb9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the species for each category\n",
    "species = ['sea sponge','sea star','crab','plankton','squid']\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# make a figure object\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "gs = gridspec.GridSpec(5, len(species))\n",
    "\n",
    "# loop through each species (column)\n",
    "for s in range(len(species)):\n",
    "\n",
    "    # loop through the image files\n",
    "    file_list = os.listdir(os.path.join('Images','validate',str(s)))[:5]\n",
    "    for file_count, file_name in enumerate(file_list):\n",
    "\n",
    "        # load the image\n",
    "        image = Image.open(os.path.join('Images','validate',str(s),file_name)).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        \n",
    "        # get the predicted class\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "        predicted_class = species[predicted.item()]\n",
    "\n",
    "        # add the image to the plot with the prediction\n",
    "        ax = fig.add_subplot(gs[file_count, s])\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(predicted_class)\n",
    "        file_count +=1\n",
    "\n",
    "plt.suptitle('Predicted Labels of All Images in the Validation Dataset after Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052353a8-e005-4af7-9131-c82acbfe6708",
   "metadata": {},
   "source": [
    "Looks pretty good - let's explore one other way we can improve our model to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa22f5e-4ce1-4498-b9aa-899155b2ab80",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "Data augmentation is the process of supplementing the training data set to include more training examples when the training examples do not exist. To create these artificial training examples, we will apply transformations to our existing set in such a way that the character of the image remains intact while the actual pixel values have changed. \n",
    "\n",
    "Since this augmentation procedure is quite common, the `torchvision` module is designed to give us a way to call upon typicial transformations. Let's have a look at a few of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae86ec-297d-4816-8aaf-7dd12a0e8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the last image from the validation set\n",
    "image = Image.open(os.path.join('Images','validate',str(s),file_name)).convert('RGB')\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "                                # ^ Add batch dimension\n",
    "\n",
    "# define a list of augmentations\n",
    "\n",
    "\n",
    "# make a figure object\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "gs = gridspec.GridSpec(2, 3)\n",
    "\n",
    "# organize the plots of each image\n",
    "ax = fig.add_subplot(gs[0,0])\n",
    "ax.imshow(image)\n",
    "ax.axis('off')\n",
    "ax.set_title('Original Image')\n",
    "\n",
    "for a, augmentation in enumerate(augmentations):\n",
    "    transformed_image = augmentation(image)\n",
    "    col = (a+1)%3\n",
    "    row = (a+1)//3\n",
    "    ax = fig.add_subplot(gs[(a+1)//3, (a+1)%3])\n",
    "    ax.imshow(transformed_image)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(augmentation_names[a])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc7eb2-2dde-40dd-afc9-dc99493d2bcf",
   "metadata": {},
   "source": [
    "Data augmentation can be used to build up more images in the training set, stored as additional files. Alternatively, we can build these random transformations directly into our data loader so that each time we load up a mini-batch, the images a modified before being passed through the network. \n",
    "\n",
    "Here, we will build in these transformations at random as the images are read in to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f568c-90be-43eb-9780-61c9858247bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training transform redefined\n",
    "train_transform_augmented = transforms.Compose([\n",
    "\n",
    "    # Apply some random flips and rotations\n",
    "    \n",
    "\n",
    "    # Randomly apply color jitters or random perspectives\n",
    "    \n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ccd71b-9b13-466f-89f2-147f87540aae",
   "metadata": {},
   "source": [
    "Now that we've re-defined our transform, let's redefine our data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba023477-3f78-4153-b659-5fbb2e6dcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data loader with the augmentation transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a9846-8a55-4806-ba93-c3d411591c9d",
   "metadata": {},
   "source": [
    "Let's try our model with the new aumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95c4ce-a62f-4d91-a8a2-9b099c53cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the model here in case cells are run \n",
    "# out of order for instructional purposes\n",
    "model = ClassificationCNN().to(device)\n",
    "\n",
    "# Adam optimizer for stochastic gradient descent\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# define the number of epochs\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "# call the training loop function we defined above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8974827-deb4-4c0d-846e-74ec63243a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, 'd-', label='Training Loss')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), test_losses, 'd-', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c0cc4-85db-4a98-ba3d-c61ef68a82b9",
   "metadata": {},
   "source": [
    "As we can see, the augmentation also helps yield a training which avoids overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf56b30-9a56-4806-af70-ebfcf3e69fed",
   "metadata": {},
   "source": [
    "As above, we can check on the performance of our model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50b873-2a64-4a64-bab4-bd4d0bc5690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the species for each category\n",
    "species = ['sea sponge','sea star','crab','plankton','squid']\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# make a figure object\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "gs = gridspec.GridSpec(5, len(species))\n",
    "\n",
    "# loop through each species (column)\n",
    "for s in range(len(species)):\n",
    "\n",
    "    # loop through the image files\n",
    "    file_list = os.listdir(os.path.join('Images','validate',str(s)))[:5]\n",
    "    for file_count, file_name in enumerate(file_list):\n",
    "\n",
    "        # load the image\n",
    "        image = Image.open(os.path.join('Images','validate',str(s),file_name)).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        \n",
    "        # get the predicted class\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "        predicted_class = species[predicted.item()]\n",
    "\n",
    "        # add the image to the plot with the prediction\n",
    "        ax = fig.add_subplot(gs[file_count, s])\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(predicted_class)\n",
    "        file_count +=1\n",
    "\n",
    "plt.suptitle('Predicted Labels of All Images in the Validation Dataset after Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c15b75-309d-475c-8a84-83934ddf4779",
   "metadata": {},
   "source": [
    "Again, we can see our model is working pretty well on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cdb932-be69-4025-a8b9-ba012392376c",
   "metadata": {},
   "source": [
    "## Try it for yourself!\n",
    "\n",
    "Can you make a better model than we made above? See if you can reduce the losses even further by implementing one of the following:\n",
    "- an additional convolutional layer\n",
    "- an additional linear layer\n",
    "- an additional drop-out\n",
    "- variations in the augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d72a5c-73d0-4929-993d-72b41fd3b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code up your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6901b-2d85-42c3-ade3-0be6b78ce556",
   "metadata": {},
   "source": [
    "When you have a good solution, check in with a neighbor to see what changes they've implemented and how their model is performing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs171",
   "language": "python",
   "name": "cs171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
